# Настройка БЯМ

Пользователи взаимодействуют с большой языковой моделью, используя промты — запросы, написанные на естественном («человеческом») языке. Промт подается на вход модели напрямую или через промежуточный интерфейс сервиса, такого как ChatGPT или Copilot. В первом случае пользователь контролирует и видит то, что получает модель, тогда как при использовании сервиса любые преобразования промта скрыты от пользователя.

При использовании прямого доступа к модели можно настроить параметры, значение которых влияет на результат генерации. В зависимости от реализации способа доступа к модели, количество и названия доступных параметров могут отличаться. Рассмотрим некоторые из них.

**Temperature** (температура) — влияет на детерминированность ответа: чем ниже температура, тем более предсказуемым становится результат. То есть при минимальной температуре в процессе генерации ответа каждый раз будет выбираться токен<sup name="ref1">[1](#note1)</sup> с наибольшим значением вероятности. Увеличение температуры сделает выбор токена более произвольным, и ответы будут более разнообразными или творческими — за счет того, что существенно увеличатся веса других возможных токенов.

Если говорить о практическом применении, то более низкое значение температуры можно использовать для таких задач, как получение фактологических ответов на вопросы, чтобы повысить достоверность и краткость результата. Для создания стихотворений или решения других творческих задач может оказаться полезной более высокая температура.

**Top_p** — этот параметр, как и температура, позволяет контролировать детерминированность ответов модели. Top_p используется для обработки первоначального набора выходных токенов и создания из него так называемого ядра выборки (nucleus sampling) — токенов для потенциального включения в ответ.

Если нужен более точный и фактологически правильный ответ, значение параметра должно быть низким. Если требуются более разнообразные ответы, значение необходимо повысить.

Общая рекомендация: изменяйте только один параметр — температуру или top_p, а не оба сразу.

**Max Length** (максимальная длина) — позволяет управлять количеством токенов, генерируемых моделью. Указание максимальной длины помогает предотвратить длинные или нерелевантные ответы, а также контролировать затраты на генерацию.

**Stop Sequences** (стоп-последовательности) — строки, которые прекращают генерацию токенов. Этот параметр позволяет контролировать длину и структуру ответа модели. Например, можно дать модели задание генерировать списки, содержащие не более 10 элементов, добавив «11» в список стоп-последовательностей. Или попробовать получить текст, который заканчивается определенной фразой или символом.

**Frequency Penalty** (штраф за частоту) — накладывает «штраф» на следующий токен, пропорциональный количеству вхождений этого токена в ответе и промте. Чем выше значение штрафа, тем ниже вероятность того, что токен появится снова. При положительном значении этот параметр должен уменьшить количество дословных повторений в ответе модели за счет того, что токены, которые появляются чаще, получают более высокий штраф.

**Presence Penalty** (штраф за присутствие) — этот параметр также применяет штраф к повторяющимся токенам, но, в отличие от штрафа за частоту, его величина одинакова для всех повторяющихся токенов. Токен, который появляется дважды, и токен, который появляется 10 раз, «наказываются» одинаково. При положительном значении этот параметр должен увеличить вероятность появления новых тем в ответах модели.

Если нужно, чтобы модель генерировала разнообразный или художественный текст, можно задать более высокий штраф за присутствие. Если требуется, чтобы модель оставалась сосредоточенной на какой-либо теме, можно попробовать снизить штраф за присутствие.

Общая рекомендация для этих параметров такая же, как и для температуры и top_p: изменяйте только один из них, а не оба сразу.

Прежде чем перейти к примерам, считаем нужным предупредить: результаты их выполнения могут различаться в зависимости от версии большой языковой модели.

[↩](#ref1) <a name="note1"><sup>1</sup>Строго говоря, модель выбирает не токен, а число, которое соответствует токену. Токен — это определенная последовательность символов естественного языка.</a>
